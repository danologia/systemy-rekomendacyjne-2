{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise, pairwise_distances\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import defaultdict\n",
    "##\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, GAE, VGAE\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.utils import from_networkx\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_to_mode(df):\n",
    "    \"\"\"\n",
    "    Replace NaN with mode in dataframe\n",
    "    \"\"\"\n",
    "    df_mode=df.mode()\n",
    "    for x in df.columns.values:\n",
    "        df.loc[:, x]=df.loc[:, x].fillna(value=df_mode[x].iloc[0])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_user(node_id, user2node):\n",
    "    if node_id in user2node.values():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/dataset_split/train.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       fit  user_id bust size  item_id  weight  rating rented for  \\\n",
       "46628  fit   345809       36b   326784  150lbs     5.0    wedding   \n",
       "18399  fit    45235       NaN  2766308  128lbs     5.0   everyday   \n",
       "12853  fit   508677       36b   254960  145lbs     3.0    wedding   \n",
       "17290  fit   117290       34a  1687082  110lbs     5.0    wedding   \n",
       "9628   fit   144767       34b   135459     NaN     4.0      party   \n",
       "\n",
       "                                             review_text body type  \\\n",
       "46628  I wore this dress for my bridal shower this pa...  athletic   \n",
       "18399  I really liked this sweater. I wore it on a fl...  athletic   \n",
       "12853  I ordered this dress and one other for a weddi...  athletic   \n",
       "17290  This dress is so fun! Was a little tight aroun...    petite   \n",
       "9628   I get nervous about gold, especially gold lace...      pear   \n",
       "\n",
       "                                          review_summary  category height  \\\n",
       "46628                          Great Bridal Shower Dress     dress  5' 6\"   \n",
       "18399                Great  sweater, perfect for travel!  cardigan  5' 6\"   \n",
       "12853  Seeing as I didn't wear this dress....not this...    sheath  5' 8\"   \n",
       "17290                                      The COLOR!!!!      gown  5' 4\"   \n",
       "9628                           Opulent but Chic and Sexy     dress  5' 5\"   \n",
       "\n",
       "       size   age       review_date  class_rating  \n",
       "46628    16  30.0       May 3, 2016             1  \n",
       "18399     8  34.0     March 7, 2017             1  \n",
       "12853    16  39.0  January 18, 2017             0  \n",
       "17290     4  33.0    March 28, 2016             1  \n",
       "9628     16  33.0  January 11, 2016             1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fit</th>\n      <th>user_id</th>\n      <th>bust size</th>\n      <th>item_id</th>\n      <th>weight</th>\n      <th>rating</th>\n      <th>rented for</th>\n      <th>review_text</th>\n      <th>body type</th>\n      <th>review_summary</th>\n      <th>category</th>\n      <th>height</th>\n      <th>size</th>\n      <th>age</th>\n      <th>review_date</th>\n      <th>class_rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>46628</th>\n      <td>fit</td>\n      <td>345809</td>\n      <td>36b</td>\n      <td>326784</td>\n      <td>150lbs</td>\n      <td>5.0</td>\n      <td>wedding</td>\n      <td>I wore this dress for my bridal shower this pa...</td>\n      <td>athletic</td>\n      <td>Great Bridal Shower Dress</td>\n      <td>dress</td>\n      <td>5' 6\"</td>\n      <td>16</td>\n      <td>30.0</td>\n      <td>May 3, 2016</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18399</th>\n      <td>fit</td>\n      <td>45235</td>\n      <td>NaN</td>\n      <td>2766308</td>\n      <td>128lbs</td>\n      <td>5.0</td>\n      <td>everyday</td>\n      <td>I really liked this sweater. I wore it on a fl...</td>\n      <td>athletic</td>\n      <td>Great  sweater, perfect for travel!</td>\n      <td>cardigan</td>\n      <td>5' 6\"</td>\n      <td>8</td>\n      <td>34.0</td>\n      <td>March 7, 2017</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12853</th>\n      <td>fit</td>\n      <td>508677</td>\n      <td>36b</td>\n      <td>254960</td>\n      <td>145lbs</td>\n      <td>3.0</td>\n      <td>wedding</td>\n      <td>I ordered this dress and one other for a weddi...</td>\n      <td>athletic</td>\n      <td>Seeing as I didn't wear this dress....not this...</td>\n      <td>sheath</td>\n      <td>5' 8\"</td>\n      <td>16</td>\n      <td>39.0</td>\n      <td>January 18, 2017</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17290</th>\n      <td>fit</td>\n      <td>117290</td>\n      <td>34a</td>\n      <td>1687082</td>\n      <td>110lbs</td>\n      <td>5.0</td>\n      <td>wedding</td>\n      <td>This dress is so fun! Was a little tight aroun...</td>\n      <td>petite</td>\n      <td>The COLOR!!!!</td>\n      <td>gown</td>\n      <td>5' 4\"</td>\n      <td>4</td>\n      <td>33.0</td>\n      <td>March 28, 2016</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9628</th>\n      <td>fit</td>\n      <td>144767</td>\n      <td>34b</td>\n      <td>135459</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>party</td>\n      <td>I get nervous about gold, especially gold lace...</td>\n      <td>pear</td>\n      <td>Opulent but Chic and Sexy</td>\n      <td>dress</td>\n      <td>5' 5\"</td>\n      <td>16</td>\n      <td>33.0</td>\n      <td>January 11, 2016</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = set(dataset['user_id'].values)\n",
    "user2node = dict(zip(user_ids, np.arange(len(user_ids))))\n",
    "\n",
    "item_ids = set(dataset['item_id'].values)\n",
    "item2node = dict(zip(item_ids, np.arange(len(item_ids)) + len(user_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_modes = dataset.groupby('item_id').apply(lambda x: x.mode().iloc[0])\n",
    "item_means = dataset.groupby('item_id').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_modes = dataset.groupby('user_id').apply(lambda x: x.mode().iloc[0])\n",
    "user_means = dataset.groupby('user_id').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(user_modes.index == user_means.index)\n",
    "assert np.all(item_modes.index == item_means.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_encoder = OneHotEncoder().fit(nan_to_mode(dataset[['rented for', 'bust size', 'body type', 'category']].copy()).values)\n",
    "numerical_scaler = StandardScaler().fit(nan_to_mode(dataset[['rating', 'size', 'age']].copy()).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5631, 144)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "user_numerical = nan_to_mode(user_means[['rating', 'size', 'age']].copy()).values\n",
    "user_numerical = numerical_scaler.transform(user_numerical)\n",
    "\n",
    "user_categorical = nan_to_mode(user_modes[['rented for', 'bust size', 'body type', 'category']].copy()).values\n",
    "user_categorical = category_encoder.transform(user_categorical).todense().A\n",
    "\n",
    "user_features = np.concatenate([user_numerical, user_categorical], axis=1)\n",
    "user_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4752, 144)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "item_numerical = nan_to_mode(item_means[['rating', 'size', 'age']].copy()).values\n",
    "item_numerical = numerical_scaler.transform(item_numerical)\n",
    "\n",
    "item_categorical = nan_to_mode(item_modes[['rented for', 'bust size', 'body type', 'category']].copy()).values\n",
    "item_categorical = category_encoder.transform(item_categorical).todense().A\n",
    "\n",
    "item_features = np.concatenate([item_numerical, item_categorical], axis=1)\n",
    "item_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "5631it [00:00, 512946.87it/s]\n",
      "4752it [00:00, 599961.85it/s]\n"
     ]
    }
   ],
   "source": [
    "graph = nx.Graph()\n",
    "\n",
    "for i, id in tqdm(enumerate(user_means.index)):\n",
    "    graph.add_node(user2node[id], features=user_features[i], id=user2node[id])\n",
    "\n",
    "for i, id in tqdm(enumerate(item_means.index)):\n",
    "    graph.add_node(item2node[id], features=item_features[i], id=item2node[id])\n",
    "\n",
    "for _, row in dataset.iterrows():\n",
    "    user_node = user2node[row['user_id']]\n",
    "    item_node = item2node[row['item_id']]\n",
    "\n",
    "    graph.add_edge(user_node, item_node, edge_weight=row['rating'] / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 84650], edge_weight=[84650], features=[10383, 144], id=[10383])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "data = from_networkx(graph)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = train_test_split_edges(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Data(edge_weight=[84650], features=[10383, 144], id=[10383], test_neg_edge_index=[2, 4232], test_pos_edge_index=[2, 4232], train_neg_adj_mask=[10383, 10383], train_pos_edge_index=[2, 71954], val_neg_edge_index=[2, 2116], val_pos_edge_index=[2, 2116])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = data.features.shape[-1]\n",
    "hidden_size = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 000, AUC: 0.7511, AP: 0.7519\n",
      "Epoch: 001, AUC: 0.7607, AP: 0.7560\n",
      "Epoch: 002, AUC: 0.7523, AP: 0.7476\n",
      "Epoch: 003, AUC: 0.7478, AP: 0.7444\n",
      "Epoch: 004, AUC: 0.7500, AP: 0.7460\n",
      "Epoch: 005, AUC: 0.7536, AP: 0.7478\n",
      "Epoch: 006, AUC: 0.7552, AP: 0.7478\n",
      "Epoch: 007, AUC: 0.7547, AP: 0.7467\n",
      "Epoch: 008, AUC: 0.7533, AP: 0.7451\n",
      "Epoch: 009, AUC: 0.7513, AP: 0.7434\n",
      "Epoch: 010, AUC: 0.7486, AP: 0.7412\n",
      "Epoch: 011, AUC: 0.7447, AP: 0.7377\n",
      "Epoch: 012, AUC: 0.7396, AP: 0.7332\n",
      "Epoch: 013, AUC: 0.7334, AP: 0.7276\n",
      "Epoch: 014, AUC: 0.7262, AP: 0.7207\n",
      "Epoch: 015, AUC: 0.7191, AP: 0.7133\n",
      "Epoch: 016, AUC: 0.7142, AP: 0.7083\n",
      "Epoch: 017, AUC: 0.7121, AP: 0.7066\n",
      "Epoch: 018, AUC: 0.7127, AP: 0.7078\n",
      "Epoch: 019, AUC: 0.7145, AP: 0.7101\n",
      "Epoch: 020, AUC: 0.7172, AP: 0.7129\n",
      "Epoch: 021, AUC: 0.7206, AP: 0.7161\n",
      "Epoch: 022, AUC: 0.7249, AP: 0.7208\n",
      "Epoch: 023, AUC: 0.7298, AP: 0.7263\n",
      "Epoch: 024, AUC: 0.7341, AP: 0.7309\n",
      "Epoch: 025, AUC: 0.7376, AP: 0.7343\n",
      "Epoch: 026, AUC: 0.7402, AP: 0.7368\n",
      "Epoch: 027, AUC: 0.7426, AP: 0.7392\n",
      "Epoch: 028, AUC: 0.7447, AP: 0.7413\n",
      "Epoch: 029, AUC: 0.7466, AP: 0.7432\n",
      "Epoch: 030, AUC: 0.7476, AP: 0.7443\n",
      "Epoch: 031, AUC: 0.7472, AP: 0.7436\n",
      "Epoch: 032, AUC: 0.7460, AP: 0.7418\n",
      "Epoch: 033, AUC: 0.7453, AP: 0.7411\n",
      "Epoch: 034, AUC: 0.7459, AP: 0.7420\n",
      "Epoch: 035, AUC: 0.7471, AP: 0.7437\n",
      "Epoch: 036, AUC: 0.7480, AP: 0.7451\n",
      "Epoch: 037, AUC: 0.7479, AP: 0.7455\n",
      "Epoch: 038, AUC: 0.7474, AP: 0.7454\n",
      "Epoch: 039, AUC: 0.7485, AP: 0.7470\n",
      "Epoch: 040, AUC: 0.7503, AP: 0.7496\n",
      "Epoch: 041, AUC: 0.7518, AP: 0.7520\n",
      "Epoch: 042, AUC: 0.7514, AP: 0.7517\n",
      "Epoch: 043, AUC: 0.7511, AP: 0.7513\n",
      "Epoch: 044, AUC: 0.7527, AP: 0.7535\n",
      "Epoch: 045, AUC: 0.7534, AP: 0.7545\n",
      "Epoch: 046, AUC: 0.7532, AP: 0.7542\n",
      "Epoch: 047, AUC: 0.7523, AP: 0.7528\n",
      "Epoch: 048, AUC: 0.7530, AP: 0.7537\n",
      "Epoch: 049, AUC: 0.7550, AP: 0.7563\n",
      "Epoch: 050, AUC: 0.7552, AP: 0.7566\n",
      "Epoch: 051, AUC: 0.7548, AP: 0.7559\n",
      "Epoch: 052, AUC: 0.7548, AP: 0.7560\n",
      "Epoch: 053, AUC: 0.7564, AP: 0.7584\n",
      "Epoch: 054, AUC: 0.7566, AP: 0.7586\n",
      "Epoch: 055, AUC: 0.7550, AP: 0.7566\n",
      "Epoch: 056, AUC: 0.7560, AP: 0.7582\n",
      "Epoch: 057, AUC: 0.7570, AP: 0.7597\n",
      "Epoch: 058, AUC: 0.7548, AP: 0.7571\n",
      "Epoch: 059, AUC: 0.7552, AP: 0.7575\n",
      "Epoch: 060, AUC: 0.7547, AP: 0.7573\n",
      "Epoch: 061, AUC: 0.7527, AP: 0.7550\n",
      "Epoch: 062, AUC: 0.7551, AP: 0.7577\n",
      "Epoch: 063, AUC: 0.7523, AP: 0.7544\n",
      "Epoch: 064, AUC: 0.7518, AP: 0.7540\n",
      "Epoch: 065, AUC: 0.7534, AP: 0.7560\n",
      "Epoch: 066, AUC: 0.7504, AP: 0.7520\n",
      "Epoch: 067, AUC: 0.7525, AP: 0.7550\n",
      "Epoch: 068, AUC: 0.7506, AP: 0.7526\n",
      "Epoch: 069, AUC: 0.7505, AP: 0.7523\n",
      "Epoch: 070, AUC: 0.7529, AP: 0.7552\n",
      "Epoch: 071, AUC: 0.7511, AP: 0.7529\n",
      "Epoch: 072, AUC: 0.7520, AP: 0.7542\n",
      "Epoch: 073, AUC: 0.7515, AP: 0.7537\n",
      "Epoch: 074, AUC: 0.7538, AP: 0.7562\n",
      "Epoch: 075, AUC: 0.7503, AP: 0.7523\n",
      "Epoch: 076, AUC: 0.7547, AP: 0.7575\n",
      "Epoch: 077, AUC: 0.7518, AP: 0.7542\n",
      "Epoch: 078, AUC: 0.7528, AP: 0.7552\n",
      "Epoch: 079, AUC: 0.7551, AP: 0.7582\n",
      "Epoch: 080, AUC: 0.7509, AP: 0.7535\n",
      "Epoch: 081, AUC: 0.7583, AP: 0.7619\n",
      "Epoch: 082, AUC: 0.7479, AP: 0.7502\n",
      "Epoch: 083, AUC: 0.7581, AP: 0.7619\n",
      "Epoch: 084, AUC: 0.7556, AP: 0.7591\n",
      "Epoch: 085, AUC: 0.7532, AP: 0.7563\n",
      "Epoch: 086, AUC: 0.7599, AP: 0.7641\n",
      "Epoch: 087, AUC: 0.7517, AP: 0.7544\n",
      "Epoch: 088, AUC: 0.7603, AP: 0.7643\n",
      "Epoch: 089, AUC: 0.7575, AP: 0.7614\n",
      "Epoch: 090, AUC: 0.7542, AP: 0.7575\n",
      "Epoch: 091, AUC: 0.7629, AP: 0.7671\n",
      "Epoch: 092, AUC: 0.7526, AP: 0.7553\n",
      "Epoch: 093, AUC: 0.7615, AP: 0.7660\n",
      "Epoch: 094, AUC: 0.7587, AP: 0.7626\n",
      "Epoch: 095, AUC: 0.7557, AP: 0.7589\n",
      "Epoch: 096, AUC: 0.7641, AP: 0.7687\n",
      "Epoch: 097, AUC: 0.7552, AP: 0.7590\n",
      "Epoch: 098, AUC: 0.7588, AP: 0.7630\n",
      "Epoch: 099, AUC: 0.7632, AP: 0.7677\n"
     ]
    }
   ],
   "source": [
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, out_channels, cached=True)\n",
    "        self.conv2 = GCNConv(out_channels, out_channels, cached=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "model = GAE(GCNEncoder(num_features, hidden_size))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(data.features.float(), data.train_pos_edge_index)\n",
    "    loss = model.recon_loss(z, data.train_pos_edge_index)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "def test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(data.features.float(), data.train_pos_edge_index)\n",
    "    return model.test(z, pos_edge_index, neg_edge_index)\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    loss = train()\n",
    "    auc, ap = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
    "    print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_popular_items = dataset.groupby('item_id').count().sort_values('fit', ascending=False)[:1000].index.values\n",
    "\n",
    "item_node_ids = list(map(lambda x: item2node[x], most_popular_items))\n",
    "user_node_ids = list(map(lambda x: user2node[x], user_ids))\n",
    "\n",
    "potential_edges = np.array(list(product(item_node_ids, user_node_ids))).transpose(1, 0)\n",
    "potential_edges = torch.tensor(potential_edges.astype(np.long)).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 5631000])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "potential_edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "z = model.encode(data.features.float(), data.train_pos_edge_index)\n",
    "#edge_probs = model.decode(z, potential_edges).detach().cpu().numpy()\n",
    "edge_probs = torch.sum(z.cpu()[potential_edges.cpu()[0]] * z.cpu()[potential_edges.cpu()[1]], dim=-1)\n",
    "edge_probs = torch.sigmoid(edge_probs).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to('cpu')\n",
    "most_probable_edges = np.argsort(edge_probs)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5631000,)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "edge_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2user = dict(map(lambda x: (x[1], x[0]), user2node.items()))\n",
    "node2item = dict(map(lambda x: (x[1], x[0]), item2node.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = dataset[['user_id', 'item_id']].values.tolist()\n",
    "transactions = map(tuple, transactions)\n",
    "transactions = set(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5631000/5631000 [1:16:34<00:00, 1225.63it/s]\n"
     ]
    }
   ],
   "source": [
    "recomendations = defaultdict(lambda: {})\n",
    "\n",
    "potential_edges = potential_edges.cpu()\n",
    "\n",
    "for edge in tqdm(most_probable_edges):\n",
    "    prob = edge_probs[edge]\n",
    "    node_from_id, node_to_id = potential_edges[:, edge].numpy()\n",
    "\n",
    "    node_from = data.id[node_from_id].item()\n",
    "    node_to = data.id[node_to_id].item()\n",
    "\n",
    "    if is_user(node_from, user2node):\n",
    "        user_id = node2user[node_from]\n",
    "        item_id = node2item[node_to]\n",
    "    else:\n",
    "        user_id = node2user[node_to]\n",
    "        item_id = node2item[node_from]\n",
    "\n",
    "    index = (user_id, item_id)\n",
    "    # Don't recomend already bought items\n",
    "    if index not in transactions:\n",
    "        if len(recomendations[str(user_id)]) < 100:\n",
    "            recomendations[str(user_id)][str(item_id)] = float(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deep_gnn.json\", 'w') as f:\n",
    "    json.dump(recomendations, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5631000, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "potential_edges.transpose(1, 0).shape"
   ]
  }
 ]
}